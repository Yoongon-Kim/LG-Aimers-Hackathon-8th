import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from llmcompressor.transformers import oneshot
from llmcompressor.modifiers.quantization import QuantizationModifier

MODEL_ID = "/home/yjjang/aimers/aimers_hackathon/base_model"
OUT_DIR = "/home/yjjang/aimers/aimers_hackathon/model"

print("[INFO] ëª¨ë¸ì„ CPUë¡œ ëª…ì‹œì  ë¡œë“œ ì¤‘ (BF16 ì •ë°€ë„ ìœ ì§€)...")
# 1. ëª¨ë¸ì„ CPUë¡œ ëª…ì‹œì  ë¡œë“œ (BF16 ì •ë°€ë„ ìœ ì§€)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, 
    device_map="cpu", 
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

print("[INFO] FP8 ë ˆì‹œí”¼ ì„¤ì • ì¤‘...")
# 2. ë ˆì‹œí”¼ ì„¤ì • (FP8)
recipe = QuantizationModifier(targets="Linear", scheme="FP8")

print("[INFO] ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
# 3. ë°ì´í„°ì…‹ ì¤€ë¹„
ds = load_dataset("LGAI-EXAONE/MANTA-1M", split="train[:256]")

def tokenize_fn(examples):
    texts = []
    for conv in examples["conversations"]:
        text = tokenizer.apply_chat_template(conv, add_generation_prompt=False, tokenize=False)
        texts.append(text)
    # ìˆ«ì ë°ì´í„°(input_ids ë“±)ë§Œ ë°˜í™˜
    return tokenizer(texts, padding="max_length", max_length=512, truncation=True)

# âš ï¸ í•µì‹¬: remove_columns ì˜µì…˜ì„ ì¶”ê°€í•˜ì—¬ ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
ds = ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)

print("[INFO] ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ (ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°ë¨)")

print("[INFO] FP8 ì–‘ìí™” ì‹¤í–‰ ì¤‘ (ì´ ê³¼ì •ì´ ìˆ˜ì‹­ ë¶„ ê±¸ë ¤ì•¼ ì •ìƒì…ë‹ˆë‹¤!)...")
# 4. ì–‘ìí™” ì‹¤í–‰ (ì´ë•Œ 1ì´ˆ ë§Œì— ëë‚˜ë©´ ì•ˆ ë©ë‹ˆë‹¤! ìˆ˜ì‹­ ë¶„ì´ ê±¸ë ¤ì•¼ ì •ìƒì…ë‹ˆë‹¤)
oneshot(
    model=model,
    dataset=ds,
    recipe=recipe,
    output_dir=OUT_DIR,
    save_compressed=True
)

print("[INFO] í† í¬ë‚˜ì´ì € ì €ì¥ ì¤‘...")
tokenizer.save_pretrained(OUT_DIR)

print(f"âœ… ì²´í¬: {OUT_DIR}/model.safetensors ìš©ëŸ‰ì´ ì¤„ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!")

# ========== ì–‘ìí™” ê²€ì¦ ==========
import os
import json

print("\n" + "="*60)
print("ğŸ” ì–‘ìí™” ê²€ì¦ ì‹œì‘")
print("="*60)

# 1. íŒŒì¼ í¬ê¸° í™•ì¸
model_size = os.path.getsize(f"{OUT_DIR}/model.safetensors") / (1024**3)
print(f"\n1ï¸âƒ£ íŒŒì¼ í¬ê¸°: {model_size:.2f} GB")
if model_size < 2.0:
    print("   âœ… ì„±ê³µ! (2.4GB â†’ {:.2f}GBë¡œ ê°ì†Œ)".format(model_size))
else:
    print("   âŒ ì‹¤íŒ¨! íŒŒì¼ì´ ì¤„ì–´ë“¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# 2. config.jsonì—ì„œ ì–‘ìí™” ì„¤ì • í™•ì¸
config_path = f"{OUT_DIR}/config.json"
with open(config_path, 'r') as f:
    config = json.load(f)

print(f"\n2ï¸âƒ£ ì–‘ìí™” ì„¤ì • í™•ì¸:")
if "quantization_config" in config:
    print("   âœ… quantization_config ë°œê²¬!")
    print(f"   ì„¤ì •: {json.dumps(config['quantization_config'], indent=2)}")
else:
    print("   âŒ quantization_configê°€ ì—†ìŠµë‹ˆë‹¤!")
    print("   í˜„ì¬ config.json í‚¤:", list(config.keys()))

print("\n" + "="*60)
if model_size < 2.0 and "quantization_config" in config:
    print("ğŸ‰ ì–‘ìí™” ì™„ë²½ ì„±ê³µ!")
else:
    print("âš ï¸ ì–‘ìí™”ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
print("="*60)